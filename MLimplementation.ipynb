{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import csv\n",
    "import os\n",
    "\n",
    "#SciKit\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPARATION\n",
    "\n",
    "Before the content of the tweets can be analyzed, we need to acquire them and convert them into a form that can be fed into our feature engineering program. \n",
    "Tweet Scraping\n",
    "Rudniy is attempting to conduct scraping with his own program, will update\n",
    "\n",
    "As of this week (July 3rd), Twitter has implemented a temporary \"view cap\" on \n",
    "\n",
    "Conversion of Tweets in .JSON format\n",
    ".JSON can be easily imported into Jupyter Notebook\n",
    "Pandas Dataframe: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html \n",
    "\n",
    "Tweet tokenizatation\n",
    "Tokenized tweets refer to the process of breaking down a tweet into individual tokens or words. Tokenization is a fundamental step in natural language processing (NLP), where the goal is to convert raw text data into a format suitable for analysis and modeling.\n",
    "\n",
    "I will use the Natural Language Toolkit (NLTK) to provide tokenizatiation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "#Get Number of Files\n",
    "\n",
    "path = \"./Tweet Datasets/collections - #jailbait\"\n",
    "\n",
    "def count_files(path):\n",
    "  count = 0\n",
    "  for filename in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, filename)):\n",
    "      count += 1\n",
    "  return count\n",
    "\n",
    "number_of_files = count_files(path)\n",
    "print(number_of_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tweet_tokenizer = TweetTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
